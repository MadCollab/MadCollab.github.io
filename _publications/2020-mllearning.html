---
layout: publication
year: 2020
pdf: https://www.sciencedirect.com/science/article/pii/S0747563219304182/pdfft?md5=df9ab59a5356b38fde2e199f1f5a66d6&pid=1-s2.0-S0747563219304182-main.pdf
title: "Teaching citizen scientists to categorize glitches using machine learning guided training"
authors:
  - Corey Jackson
  - Carsten Østerlund
  - Kevin Crowston
  - Mahboobeh Harandi
  - Sarah Allen
  - Sara Bahaadini
  - Scotty Coughlin
  - Vicky Kalogera
  - Aggelos Katsaggelos
  - Shane Larson
  - Neda Rohani
  - Joshua Smith
  - Laura Trouille
  - Michael Zevin
tags:
  - Learning
  - Citizen science
venue: Computers in Human Behavior
venue_location:
venue_tags:
  - CHB
venue_url: https://www.sciencedirect.com/journal/computers-in-human-behavior
type:
  - Journal
---

Existing literature points to scaffolded training as an effective yet resource-intensive approach to help newcomers learn and stay motivated. Experts need to select relevant learning materials and continuously assess learners' progress. Peer production communities such as Wikipedia and Open Source Software Development projects face the additional problem of turning volunteers into productive participants as soon as possible. To address these challenges, we designed and tested a training regime combining scaffolded instruction and machine learning to select learning materials and gradually introduces new materials to individuals as their competences improve. We evaluated the training regime on 386 participants that contribute to Gravity Spy, an online citizen science project where people are asked to categorize glitches to assist scientists in the search for gravitational waves. Volunteers were assigned to one of two conditions; (1) a machine learning guided training (MLGT) system that continuously assesses volunteers skill level and adjusts the learning materials or (2) an unscaffolded training program where all learning materials were administered at once. Our analysis revealed that volunteers in the MLGT condition were more accurate on the categorization task (an average accuracy of 90% vs. 54%), executed more tasks (an average of 228 vs. 121 tasks), and were retained for a longer period (an average of 2.5 vs. 2 sessions) than volunteers in the unscaffolded training. The results suggest that MLGT is an effective pedagogical approach for training volunteers in categorization tasks and increases volunteers’ motivation.
